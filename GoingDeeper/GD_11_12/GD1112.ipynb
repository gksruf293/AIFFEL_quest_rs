{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "1.3.3\n",
      "3.4.3\n",
      "2.0.9\n",
      "2.2.1\n",
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(plt.__version__)\n",
    "print(json.__version__)\n",
    "print(re.__version__)\n",
    "\n",
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc0b10",
   "metadata": {},
   "source": [
    "### Tokenizer 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb066c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = os.getenv('HOME')+'/aiffel/bert_pretrain/models/ko_8000'\n",
    "vocab_size = 8000\n",
    "#spm.SentencePieceTrainer.train(f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7} --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187796f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94210a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37bf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e1fb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2a51b",
   "metadata": {},
   "source": [
    "### MASK 생성\n",
    "\n",
    "\\_로 시작하는 단어는 단어의 시작\n",
    "\n",
    "전체 단어 중 15%를 마스킹,\n",
    "\n",
    "마스킹 별로 80%는 [MASK], 10%는 랜덤한 토큰, 나머지 10%는 원래 토큰을 그대로 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b580e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c11ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf818d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "\n",
    "            #######################################\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            \n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2a69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    save_pretrain_instances(out_f, doc)\n",
    "                    doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)    \n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c85e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "#make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064159ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0486f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdadf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd13e67c008412ea95202c9550bc7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1630496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_96/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_96/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2da7e8",
   "metadata": {},
   "source": [
    "5번token([CLS])으로 시작해서 4번 token([SEP])으로 끝나는 데이터들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3ab8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5   10 1605 ...   16 3599    4]\n",
      " [   5    6    6 ... 4313 4290    4]\n",
      " [   5 3873 3667 ...   76  955    4]\n",
      " ...\n",
      " [   5 3662    6 ... 3650 3731    4]\n",
      " [   5  108 3663 ...    0    0    0]\n",
      " [   5   55 3674 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(pre_train_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab1266",
   "metadata": {},
   "source": [
    "데이터 순서가 바뀌어있으면 0, 순서가 올바르면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f617bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(pre_train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a590de4",
   "metadata": {},
   "source": [
    "### 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6235111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61842de8",
   "metadata": {},
   "source": [
    "GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb0cae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a81d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57975961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f7a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51dac02",
   "metadata": {},
   "source": [
    "Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90ae09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc819735",
   "metadata": {},
   "source": [
    "ScaleDotProductAttention + MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6116294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5d7c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        # (bs, n_head, Q_len, d_head)\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)\n",
    "        # transpose and liner\n",
    "        # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.transpose(attn_out, perm=[0, 2, 1, 3])\n",
    "        # (bs, Q_len, d_model)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, -1, self.d_model])\n",
    "        # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06600456",
   "metadata": {},
   "source": [
    "transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c265d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac13f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466df58",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e47c8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d23ed",
   "metadata": {},
   "source": [
    "pretrain용 BERT 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02ff3ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "091175cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c9626",
   "metadata": {},
   "source": [
    "### 4M model\n",
    "\n",
    "lms 노드에 있는 기본 프리셋으로 진행했을 때, 4M의 모델이 만들어졌습니다.\n",
    "\n",
    "학습시간은 총 약 10시간 정도 걸렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1799ba30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0914239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 3s 10ms/step - loss: 9.6809 - nsp_loss: 0.6697 - mlm_loss: 9.0112 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.4637 - nsp_loss: 0.5138 - mlm_loss: 7.9499 - nsp_acc: 0.9000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7b2bfab33130>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a4ac1",
   "metadata": {},
   "source": [
    "### pretrain 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8e1343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9067005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "779ac3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d439803a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d78059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6043f24",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d14f9be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 254770\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fccf249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25477/25477 [==============================] - 3453s 135ms/step - loss: 15.4352 - nsp_loss: 0.3995 - mlm_loss: 15.0356 - nsp_acc: 0.9070 - mlm_lm_acc: 0.1656\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.16556, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "25477/25477 [==============================] - 3456s 136ms/step - loss: 11.0104 - nsp_loss: 0.3403 - mlm_loss: 10.6701 - nsp_acc: 0.9721 - mlm_lm_acc: 0.2901\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.16556 to 0.29011, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "25477/25477 [==============================] - 3456s 136ms/step - loss: 10.3176 - nsp_loss: 0.3324 - mlm_loss: 9.9852 - nsp_acc: 0.9803 - mlm_lm_acc: 0.3203\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.29011 to 0.32031, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "25477/25477 [==============================] - 3454s 136ms/step - loss: 10.0016 - nsp_loss: 0.3273 - mlm_loss: 9.6743 - nsp_acc: 0.9856 - mlm_lm_acc: 0.3352\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.32031 to 0.33517, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "25477/25477 [==============================] - 3454s 136ms/step - loss: 9.7973 - nsp_loss: 0.3248 - mlm_loss: 9.4725 - nsp_acc: 0.9882 - mlm_lm_acc: 0.3447\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.33517 to 0.34467, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "25477/25477 [==============================] - 3456s 136ms/step - loss: 9.6380 - nsp_loss: 0.3234 - mlm_loss: 9.3147 - nsp_acc: 0.9896 - mlm_lm_acc: 0.3522\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.34467 to 0.35218, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "25477/25477 [==============================] - 3450s 135ms/step - loss: 9.5057 - nsp_loss: 0.3223 - mlm_loss: 9.1834 - nsp_acc: 0.9908 - mlm_lm_acc: 0.3584\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.35218 to 0.35839, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "25477/25477 [==============================] - 3451s 135ms/step - loss: 9.3934 - nsp_loss: 0.3213 - mlm_loss: 9.0721 - nsp_acc: 0.9918 - mlm_lm_acc: 0.3636\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.35839 to 0.36359, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "25477/25477 [==============================] - 3449s 135ms/step - loss: 9.3078 - nsp_loss: 0.3207 - mlm_loss: 8.9872 - nsp_acc: 0.9924 - mlm_lm_acc: 0.3675\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.36359 to 0.36754, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "25477/25477 [==============================] - 3450s 135ms/step - loss: 9.2584 - nsp_loss: 0.3203 - mlm_loss: 8.9382 - nsp_acc: 0.9929 - mlm_lm_acc: 0.3697\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.36754 to 0.36975, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=save_weights)\n",
    "# 모델 인자에는 inputs, labels, epochs, batch size, callback 이 필요해요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341843",
   "metadata": {},
   "source": [
    "nsp의 경우 거의 1.0의 acc을 가지고,\n",
    "mlm의 경우 acc은 약 0.4, loss는 약 9정도로 수렴하는 모습을 보였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f660b54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35711db1",
   "metadata": {},
   "source": [
    "### 1M 모델\n",
    "\n",
    "d_model, n_head, d_head, d_ff, n_layer의 값을 전체적으로 줄여 1M 모델을 만들어\n",
    "\n",
    "학습을 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82ddfdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 96,\n",
       " 'n_head': 2,\n",
       " 'd_head': 48,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 256,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 2,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 96, \"n_head\": 2, \"d_head\": 48, \"dropout\": 0.1, \"d_ff\": 256, \"layernorm_epsilon\": 0.001, \"n_layer\": 2, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bce55df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 96), (None,  967904      enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            9504        bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 977,408\n",
      "Trainable params: 977,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928df28",
   "metadata": {},
   "source": [
    "모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1fb9be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 254770\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "75071ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25477/25477 [==============================] - 1505s 59ms/step - loss: 17.8101 - nsp_loss: 0.4506 - mlm_loss: 17.3594 - nsp_acc: 0.8473 - mlm_lm_acc: 0.0962\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.09616, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 2/10\n",
      "25477/25477 [==============================] - 1505s 59ms/step - loss: 13.6342 - nsp_loss: 0.3687 - mlm_loss: 13.2656 - nsp_acc: 0.9425 - mlm_lm_acc: 0.1906\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.09616 to 0.19060, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 3/10\n",
      "25477/25477 [==============================] - 1504s 59ms/step - loss: 12.5333 - nsp_loss: 0.3557 - mlm_loss: 12.1776 - nsp_acc: 0.9560 - mlm_lm_acc: 0.2259\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.19060 to 0.22591, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 4/10\n",
      "25477/25477 [==============================] - 1502s 59ms/step - loss: 12.2549 - nsp_loss: 0.3511 - mlm_loss: 11.9038 - nsp_acc: 0.9608 - mlm_lm_acc: 0.2353\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.22591 to 0.23533, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 5/10\n",
      "25477/25477 [==============================] - 1501s 59ms/step - loss: 12.1269 - nsp_loss: 0.3489 - mlm_loss: 11.7780 - nsp_acc: 0.9631 - mlm_lm_acc: 0.2398\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.23533 to 0.23984, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 6/10\n",
      "25477/25477 [==============================] - 1501s 59ms/step - loss: 12.0503 - nsp_loss: 0.3473 - mlm_loss: 11.7029 - nsp_acc: 0.9648 - mlm_lm_acc: 0.2425\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.23984 to 0.24251, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 7/10\n",
      "25477/25477 [==============================] - 1501s 59ms/step - loss: 11.9936 - nsp_loss: 0.3459 - mlm_loss: 11.6477 - nsp_acc: 0.9663 - mlm_lm_acc: 0.2445\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.24251 to 0.24452, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 8/10\n",
      "25477/25477 [==============================] - 1502s 59ms/step - loss: 11.9476 - nsp_loss: 0.3448 - mlm_loss: 11.6028 - nsp_acc: 0.9674 - mlm_lm_acc: 0.2461\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.24452 to 0.24606, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 9/10\n",
      "25477/25477 [==============================] - 1502s 59ms/step - loss: 11.9151 - nsp_loss: 0.3442 - mlm_loss: 11.5710 - nsp_acc: 0.9680 - mlm_lm_acc: 0.2472\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.24606 to 0.24722, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n",
      "Epoch 10/10\n",
      "25477/25477 [==============================] - 1504s 59ms/step - loss: 11.8963 - nsp_loss: 0.3435 - mlm_loss: 11.5528 - nsp_acc: 0.9688 - mlm_lm_acc: 0.2477\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.24722 to 0.24773, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_1M.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train_1M.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=save_weights)\n",
    "# 모델 인자에는 inputs, labels, epochs, batch size, callback 이 필요해요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49640e6a",
   "metadata": {},
   "source": [
    "4M 모델보다 mlm loss의 값이 약 3정도 높은 곳에서 수렴하고, acc도 0.1 차이가 발생했습니다.\n",
    "\n",
    "nsp는 둘다 높은 acc을 가졌습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a275442",
   "metadata": {},
   "source": [
    "![](images/output1.png)\n",
    "![](images/output2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c7947",
   "metadata": {},
   "source": [
    "### 4M 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "840fd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_model.load_weights(f\"{model_dir}/bert_pre_train.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9aa50c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [[   5  199 3657 3853 3657   77 3608  114  368  687  500 3633   13 3968\n",
      "  3613 3596 7213 3610  516 4265 3597  201 3630  404  254 3600  391 4192\n",
      "    25  938 3982 3821   64 4192   25   64 3981 2848  764 3672 3602 2288\n",
      "    25 4357 3622  456 3741 3683   80 4809 3600    4  516 3747 4354   45\n",
      "  4014   24 5531 3601 1130 2574 3211 3727 3315 4221 3612  266 3600   60\n",
      "  3646   34 4220 3607  239 4289  851  290  119   25   93 1316 3815 3738\n",
      "   368 2715  736 3600   13 3784 3815 1144 3609 2645 3598 3781    4]]\n",
      "Segment IDs: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.26894322, 0.73105675]], dtype=float32),\n",
       " array([[[2.40792608e-10, 5.85199305e-05, 2.46435150e-10, ...,\n",
       "          3.28789298e-07, 1.31485436e-07, 6.63250432e-07],\n",
       "         [1.73084227e-15, 3.03982745e-07, 1.75043657e-15, ...,\n",
       "          6.36599884e-10, 1.18749316e-10, 7.19960203e-10],\n",
       "         [2.13036172e-14, 1.94850045e-07, 2.16941469e-14, ...,\n",
       "          1.12385867e-09, 8.97136254e-11, 7.04412795e-09],\n",
       "         ...,\n",
       "         [5.03130900e-16, 1.05240258e-06, 5.11772013e-16, ...,\n",
       "          3.06713786e-11, 1.70651479e-10, 2.94157511e-11],\n",
       "         [2.84203433e-13, 1.11571666e-07, 2.89997410e-13, ...,\n",
       "          5.11052800e-09, 6.06360295e-09, 2.86458213e-09],\n",
       "         [1.03151009e-10, 1.25211245e-05, 1.05275594e-10, ...,\n",
       "          1.89520776e-07, 1.89051079e-07, 7.49751507e-07]]], dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "test_text = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "\n",
    "enc_token = [vocab.piece_to_id(p) for p in test_text]\n",
    "segment = []\n",
    "seg_id = 0\n",
    "for token in test_text:\n",
    "    segment.append(seg_id)\n",
    "    if token == \"[SEP]\":\n",
    "        seg_id = 1  # 첫 [SEP] 이후부터는 segment 1로 전환\n",
    "\n",
    "input_ids = np.array([enc_token], dtype=np.int32)\n",
    "segment_ids = np.array([segment], dtype=np.int32)\n",
    "\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Segment IDs:\", segment_ids)\n",
    "\n",
    "pre_train_model.predict((input_ids, segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ffcff302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_segment_b_and_predict(enc_token, segment, model, vocab):\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. 준비\n",
    "    input_ids = np.array([enc_token], dtype=np.int32)      # (1, seq_len)\n",
    "    segment_ids = np.array([segment], dtype=np.int32)      # (1, seq_len)\n",
    "\n",
    "    # 2. 마스킹\n",
    "    mask_token_id = vocab.piece_to_id(\"[MASK]\")\n",
    "    masked_input_ids = input_ids.copy()\n",
    "    masked_input_ids[0][segment_ids[0] == 1] = mask_token_id  # B 영역만 마스킹\n",
    "\n",
    "    # 3. 예측\n",
    "    nsp_pred, mlm_pred = model.predict((masked_input_ids, segment_ids))\n",
    "\n",
    "    # 4. 복원\n",
    "    recovered_ids = masked_input_ids.copy()\n",
    "    for i in range(segment_ids.shape[1]):\n",
    "        if segment_ids[0][i] == 1:  # segment B\n",
    "            recovered_ids[0][i] = np.argmax(mlm_pred[0][i])\n",
    "\n",
    "    # 5. 토큰 디코딩\n",
    "    recovered_tokens = [vocab.id_to_piece(int(tok)) for tok in recovered_ids[0]]\n",
    "\n",
    "    return recovered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17102d3",
   "metadata": {},
   "source": [
    "B 문장을 전부 마스킹했을 때 결과 분석\n",
    "\n",
    "훈련 때 mask가 전부 가려진 것을 훈련해본 적이 없어서 낮은 성능을 보이는 것으로 분석된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00aad587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복원된 문장: [CLS] 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP] 노가리리리리)))))))))))))))(((((-----------------((\n"
     ]
    }
   ],
   "source": [
    "recovered_tokens = mask_segment_b_and_predict(enc_token, segment, pre_train_model, vocab)\n",
    "print(\"복원된 문장:\", \"\".join(recovered_tokens).replace(\"▁\", \" \").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6173e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_segment_b_word_level_from_tokenized(test_text, model, vocab, mask_prob=0.15, mask_ab='all'):\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    input_ids = [vocab.piece_to_id(t) for t in test_text]\n",
    "    sep_index_1 = test_text.index(\"[SEP]\")\n",
    "    sep_index_2 = len(test_text) - 1  # 마지막 [SEP]\n",
    "\n",
    "    segment_ids = [0] * (sep_index_1 + 1) + [1] * (len(test_text) - sep_index_1 - 1)\n",
    "\n",
    "    # 단어 단위 후보군 (A, B, all 구분)\n",
    "    cand_idx = []\n",
    "    if mask_ab is not None:\n",
    "        for i, token in enumerate(test_text):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            if (mask_ab == 'a' and segment_ids[i] != 0) or \\\n",
    "               (mask_ab == 'b' and segment_ids[i] != 1):\n",
    "                continue\n",
    "            if cand_idx and not token.startswith(\"▁\"):\n",
    "                cand_idx[-1].append(i)\n",
    "            else:\n",
    "                cand_idx.append([i])\n",
    "\n",
    "    # 마스킹 적용\n",
    "    masked_input_ids = input_ids.copy()\n",
    "    if cand_idx:\n",
    "        num_to_mask = max(1, int(len(cand_idx) * mask_prob))\n",
    "        masked_indices = random.sample(cand_idx, num_to_mask)\n",
    "        mask_token_id = vocab.piece_to_id(\"[MASK]\")\n",
    "        for index_group in masked_indices:\n",
    "            for idx in index_group:\n",
    "                masked_input_ids[idx] = mask_token_id\n",
    "\n",
    "    # 예측\n",
    "    input_ids_np = np.array([masked_input_ids], dtype=np.int32)\n",
    "    segment_ids_np = np.array([segment_ids], dtype=np.int32)\n",
    "    nsp_pred, mlm_pred = model.predict((input_ids_np, segment_ids_np))\n",
    "\n",
    "    # NSP 확률 추출 (두 문장이 연결될 확률)\n",
    "    is_next_prob = float(nsp_pred[0][1])  # NSP: [not_next, is_next]\n",
    "\n",
    "    # 복원\n",
    "    recovered_ids = masked_input_ids.copy()\n",
    "    if cand_idx:\n",
    "        for index_group in masked_indices:\n",
    "            for idx in index_group:\n",
    "                recovered_ids[idx] = np.argmax(mlm_pred[0, idx])\n",
    "\n",
    "    # 디코딩\n",
    "    masked_pieces = [vocab.id_to_piece(int(i)) for i in masked_input_ids]\n",
    "    recovered_pieces = [vocab.id_to_piece(int(i)) for i in recovered_ids]\n",
    "\n",
    "    # A/B 분리\n",
    "    masked_a = \"\".join(masked_pieces[0:sep_index_1+1]).replace(\"▁\", \" \").strip()\n",
    "    masked_b = \"\".join(masked_pieces[sep_index_1+1:sep_index_2+1]).replace(\"▁\", \" \").strip()\n",
    "    recovered_a = \"\".join(recovered_pieces[0:sep_index_1+1]).replace(\"▁\", \" \").strip()\n",
    "    recovered_b = \"\".join(recovered_pieces[sep_index_1+1:sep_index_2+1]).replace(\"▁\", \" \").strip()\n",
    "\n",
    "    return masked_a, masked_b, recovered_a, recovered_b, is_next_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea44a81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 A 문장:\n",
      " 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
      "원본 B 문장:\n",
      " 손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\n",
      "\n",
      " 마스킹 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은[MASK][MASK][MASK] 손님이 많아 첫[MASK][MASK] 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      " 마스킹 B 문장:\n",
      " 손바닥[MASK][MASK] 기쁨의 눈물이 흘러 컬컬한 목에 모주[MASK][MASK][MASK][MASK][MASK] 몇 달 포 전부터 콜록거리는 아내 생각에 그토록[MASK][MASK] 싶다던[SEP]\n",
      "\n",
      " 복원된 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은 그째. 손님이 많아 첫 날은 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      "복원된 B 문장:\n",
      " 손바닥 의 기쁨의 눈물이 흘러 컬컬한 목에 모주 을들다. 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 살고 싶다던[SEP]\n",
      "\n",
      " NSP 확률 (두 문장이 이어질 확률): 0.7310\n"
     ]
    }
   ],
   "source": [
    "masked_a, masked_b, recovered_a, recovered_b, is_next_prob = mask_segment_b_word_level_from_tokenized(\n",
    "    test_text, pre_train_model, vocab, mask_prob=0.15, mask_ab='all'\n",
    ")\n",
    "\n",
    "print(\"원본 A 문장:\\n\", string_a)\n",
    "print(\"원본 B 문장:\\n\", string_b)\n",
    "print(\"\\n 마스킹 A 문장:\\n\", masked_a)\n",
    "print(\" 마스킹 B 문장:\\n\", masked_b)\n",
    "print(\"\\n 복원된 A 문장:\\n\", recovered_a)\n",
    "print(\"복원된 B 문장:\\n\", recovered_b)\n",
    "\n",
    "print(f\"\\n NSP 확률 (두 문장이 이어질 확률): {is_next_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e97cbe",
   "metadata": {},
   "source": [
    "결과 분석\n",
    "\n",
    "mlm의 경우 단어 단위로는 생성이 잘 되지만, 문장 단위로는 어법저으로 어색한 부분이 발생하는 모습입니다.\n",
    "\n",
    "nsp의 경우 0.7정도로 높은 편향을 가지지는 않지만, 예측의 방향성은 올바르게 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f82adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [[   5    8 3829   26 4036 1057  303 3649  138 4362 3688 3622  865 3788\n",
      "  3696 3703  401 3648 3637   13 5378 4360   26 4356 3669  998 3607 3596\n",
      "  6281 3598  388 3697  324 3596 5047 3921   21 4059 1130 3638 3623 3669\n",
      "    77 4035 3696 3609   80 4102 3612 3182  329  174 4001 3614 3764 3804\n",
      "   150 3804    4  114 1130 3772 3600 1599  848 1130 3772 3600   58  126\n",
      "  3870  382 3684  868  141 4547 1995 3867   58 3602 3596 4044  481 2149\n",
      "  3596 5609 3908 3628 1599 3602  687 2106 2149  207 3908 3628   13 2549\n",
      "    58 3601 1911 3597 3364 3263 1911 3597   62 3667 1816  313 3600  435\n",
      "   150 3630 3769    4]]\n",
      "Segment IDs: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.64812565, 0.35187435]], dtype=float32),\n",
       " array([[[7.60500551e-11, 1.18901640e-04, 7.71229816e-11, ...,\n",
       "          2.43637345e-07, 8.37495861e-07, 6.65926734e-07],\n",
       "         [1.81994600e-15, 5.53422197e-08, 1.85019312e-15, ...,\n",
       "          1.00322251e-09, 3.86792959e-10, 3.49805768e-10],\n",
       "         [2.10490669e-14, 3.02342733e-06, 2.13275527e-14, ...,\n",
       "          2.34534642e-10, 2.50348464e-10, 1.21330140e-10],\n",
       "         ...,\n",
       "         [1.45606776e-14, 4.63521064e-06, 1.48587129e-14, ...,\n",
       "          3.07598863e-10, 1.15875343e-09, 4.07584660e-10],\n",
       "         [1.16434735e-13, 1.38996329e-05, 1.18075837e-13, ...,\n",
       "          5.57978819e-09, 7.10783121e-09, 2.61102251e-09],\n",
       "         [5.66682708e-11, 1.19636454e-04, 5.76326278e-11, ...,\n",
       "          1.35769554e-07, 1.71141409e-07, 2.91211308e-07]]], dtype=float32))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_a = \"이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까\"\n",
    "string_b = \"눈 비비며 빼꼼히 창밖을 내다보니 삼삼오오 아이들은 재잘대며 학교 가고 산책 갔다 오시는 아버지의 양손에는 효과를 알 수 없는 약수가 하나 가득\"\n",
    "string_c = \"내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요\"\n",
    "test_text = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_c) + [\"[SEP]\"]\n",
    "\n",
    "enc_token = [vocab.piece_to_id(p) for p in test_text]\n",
    "segment = []\n",
    "seg_id = 0\n",
    "for token in test_text:\n",
    "    segment.append(seg_id)\n",
    "    if token == \"[SEP]\":\n",
    "        seg_id = 1  # 첫 [SEP] 이후부터는 segment 1로 전환\n",
    "\n",
    "input_ids = np.array([enc_token], dtype=np.int32)\n",
    "segment_ids = np.array([segment], dtype=np.int32)\n",
    "\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Segment IDs:\", segment_ids)\n",
    "\n",
    "pre_train_model.predict((input_ids, segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e1f892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복원된 문장: [CLS] 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까[SEP] 오리리리 ((()))))((((((-----------------((((((((((((((((((())\n"
     ]
    }
   ],
   "source": [
    "recovered_tokens = mask_segment_b_and_predict(enc_token, segment, pre_train_model, vocab)\n",
    "print(\"복원된 문장:\", \"\".join(recovered_tokens).replace(\"▁\", \" \").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3653bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 A 문장:\n",
      " 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까\n",
      "원본 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요\n",
      "\n",
      " 마스킹 A 문장:\n",
      " [CLS] 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까[SEP]\n",
      " 마스킹 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요[SEP]\n",
      "\n",
      " 복원된 A 문장:\n",
      " [CLS] 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까[SEP]\n",
      "복원된 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요[SEP]\n",
      "\n",
      " NSP 확률 (두 문장이 이어질 확률): 0.3519\n"
     ]
    }
   ],
   "source": [
    "masked_a, masked_b, recovered_a, recovered_b, is_next_prob = mask_segment_b_word_level_from_tokenized(\n",
    "    test_text, pre_train_model, vocab, mask_prob=0.1, mask_ab=None,\n",
    ")\n",
    "\n",
    "print(\"원본 A 문장:\\n\", string_a)\n",
    "print(\"원본 B 문장:\\n\", string_c)\n",
    "print(\"\\n 마스킹 A 문장:\\n\", masked_a)\n",
    "print(\" 마스킹 B 문장:\\n\", masked_b)\n",
    "print(\"\\n 복원된 A 문장:\\n\", recovered_a)\n",
    "print(\"복원된 B 문장:\\n\", recovered_b)\n",
    "\n",
    "print(f\"\\n NSP 확률 (두 문장이 이어질 확률): {is_next_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de76b4e",
   "metadata": {},
   "source": [
    "서로 다른 도메인의 문장을 사용하여 nsp를 추론해본 경우\n",
    "\n",
    "올바른 방향으로 잘못된 연결을 잘 찾아냈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0ff40f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 A 문장:\n",
      " 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까\n",
      "원본 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요\n",
      "\n",
      " 마스킹 A 문장:\n",
      " [CLS] 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게[MASK][MASK] 깬다[MASK][MASK] 하나 햇살 가득 눈부시게 비쳐오고[MASK][MASK][MASK] 냉기에[MASK][MASK][MASK][MASK][MASK] 말까[SEP]\n",
      " 마스킹 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요[SEP]\n",
      "\n",
      " 복원된 A 문장:\n",
      " [CLS] 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 나을 깬다 나까 하나 햇살 가득 눈부시게 비쳐오고 다. 냉기에 려다오지 말까[SEP]\n",
      "복원된 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요[SEP]\n",
      "\n",
      " NSP 확률 (두 문장이 이어질 확률): 0.5129\n"
     ]
    }
   ],
   "source": [
    "masked_a, masked_b, recovered_a, recovered_b, is_next_prob = mask_segment_b_word_level_from_tokenized(\n",
    "    test_text, pre_train_model, vocab, mask_prob=0.1, mask_ab='all',\n",
    ")\n",
    "\n",
    "print(\"원본 A 문장:\\n\", string_a)\n",
    "print(\"원본 B 문장:\\n\", string_c)\n",
    "print(\"\\n 마스킹 A 문장:\\n\", masked_a)\n",
    "print(\" 마스킹 B 문장:\\n\", masked_b)\n",
    "print(\"\\n 복원된 A 문장:\\n\", recovered_a)\n",
    "print(\"복원된 B 문장:\\n\", recovered_b)\n",
    "\n",
    "print(f\"\\n NSP 확률 (두 문장이 이어질 확률): {is_next_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121aa8b3",
   "metadata": {},
   "source": [
    "마스킹을 적용한 경우, nsp 확률을 약 0.5로 분석하여 잘 구별하지 못하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79da88",
   "metadata": {},
   "source": [
    "### 1M 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b1672cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_model.load_weights(f\"{model_dir}/bert_pre_train_1M.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e6dafceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [[   5  199 3657 3853 3657   77 3608  114  368  687  500 3633   13 3968\n",
      "  3613 3596 7213 3610  516 4265 3597  201 3630  404  254 3600  391 4192\n",
      "    25  938 3982 3821   64 4192   25   64 3981 2848  764 3672 3602 2288\n",
      "    25 4357 3622  456 3741 3683   80 4809 3600    4  516 3747 4354   45\n",
      "  4014   24 5531 3601 1130 2574 3211 3727 3315 4221 3612  266 3600   60\n",
      "  3646   34 4220 3607  239 4289  851  290  119   25   93 1316 3815 3738\n",
      "   368 2715  736 3600   13 3784 3815 1144 3609 2645 3598 3781    4]]\n",
      "Segment IDs: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.69621384, 0.30378616]], dtype=float32),\n",
       " array([[[1.09724079e-13, 1.38291653e-04, 1.11115334e-13, ...,\n",
       "          1.43784774e-07, 4.13300199e-07, 5.77842400e-07],\n",
       "         [1.92372035e-16, 3.29452462e-07, 1.91089918e-16, ...,\n",
       "          2.06745024e-10, 3.91467345e-11, 1.52085261e-10],\n",
       "         [1.29273771e-16, 5.11932114e-07, 1.26595559e-16, ...,\n",
       "          1.60170366e-09, 1.01234021e-09, 5.94907290e-10],\n",
       "         ...,\n",
       "         [7.69384195e-21, 6.95987069e-07, 7.39317068e-21, ...,\n",
       "          1.36436855e-13, 3.55653712e-13, 7.62274765e-14],\n",
       "         [3.05303735e-16, 2.44709372e-06, 2.97510952e-16, ...,\n",
       "          1.55383623e-10, 1.28010741e-10, 7.03881953e-10],\n",
       "         [1.76578294e-12, 4.76002526e-07, 1.75158369e-12, ...,\n",
       "          1.07106697e-08, 6.51183196e-09, 1.80109776e-08]]], dtype=float32))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "test_text = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "\n",
    "enc_token = [vocab.piece_to_id(p) for p in test_text]\n",
    "segment = []\n",
    "seg_id = 0\n",
    "for token in test_text:\n",
    "    segment.append(seg_id)\n",
    "    if token == \"[SEP]\":\n",
    "        seg_id = 1  # 첫 [SEP] 이후부터는 segment 1로 전환\n",
    "\n",
    "input_ids = np.array([enc_token], dtype=np.int32)\n",
    "segment_ids = np.array([segment], dtype=np.int32)\n",
    "\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Segment IDs:\", segment_ids)\n",
    "\n",
    "pre_train_model.predict((input_ids, segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c4aa0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 A 문장:\n",
      " 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
      "원본 B 문장:\n",
      " 손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\n",
      "\n",
      " 마스킹 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      " 마스킹 B 문장:\n",
      " [MASK][MASK][MASK][MASK][MASK] 기쁨의[MASK][MASK] 흘러 컬컬한 목에 모주 한잔을 적셔[MASK] 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던[SEP]\n",
      "\n",
      " 복원된 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      "복원된 B 문장:\n",
      " 《리리리의 기쁨의 날을 흘러 컬컬한 목에 모주 한잔을 적셔 두 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던[SEP]\n",
      "\n",
      " NSP 확률 (두 문장이 이어질 확률): 0.5930\n"
     ]
    }
   ],
   "source": [
    "masked_a, masked_b, recovered_a, recovered_b, is_next_prob = mask_segment_b_word_level_from_tokenized(\n",
    "    test_text, pre_train_model, vocab, mask_prob=0.1, mask_ab='all',\n",
    ")\n",
    "\n",
    "print(\"원본 A 문장:\\n\", string_a)\n",
    "print(\"원본 B 문장:\\n\", string_b)\n",
    "print(\"\\n 마스킹 A 문장:\\n\", masked_a)\n",
    "print(\" 마스킹 B 문장:\\n\", masked_b)\n",
    "print(\"\\n 복원된 A 문장:\\n\", recovered_a)\n",
    "print(\"복원된 B 문장:\\n\", recovered_b)\n",
    "\n",
    "print(f\"\\n NSP 확률 (두 문장이 이어질 확률): {is_next_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec500e9",
   "metadata": {},
   "source": [
    "1M 모델의 경우 마스크가 적용된 문장의 nsp를 0.59 정도의 값으로 비교적 잘 잡아내지 못하는 모습입니다.\n",
    "\n",
    "mlm의 경우도 4M 모델보다 품질이 낮은 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c28b0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 A 문장:\n",
      " 이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까\n",
      "원본 B 문장:\n",
      " 내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요\n",
      "\n",
      " 마스킹 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에[MASK][MASK] 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      " 마스킹 B 문장:\n",
      " 손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에[MASK][MASK] 한잔을[MASK][MASK] 몇 달 포 전부터 콜록거리는 아내 생각에[MASK][MASK][MASK] 먹고 싶다던[SEP]\n",
      "\n",
      " 복원된 A 문장:\n",
      " [CLS] 추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 달. 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에[SEP]\n",
      "복원된 B 문장:\n",
      " 손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 달. 한잔을 먹. 몇 달 포 전부터 콜록거리는 아내 생각에 다을 먹고 싶다던[SEP]\n",
      "\n",
      " NSP 확률 (두 문장이 이어질 확률): 0.4622\n"
     ]
    }
   ],
   "source": [
    "string_a = \"이른 아침 작은 새들 노랫소리 들려오면 언제나 그랬듯 아쉽게 잠을 깬다 창문 하나 햇살 가득 눈부시게 비쳐오고 서늘한 냉기에 재채기할까 말까\"\n",
    "string_b = \"눈 비비며 빼꼼히 창밖을 내다보니 삼삼오오 아이들은 재잘대며 학교 가고 산책 갔다 오시는 아버지의 양손에는 효과를 알 수 없는 약수가 하나 가득\"\n",
    "string_c = \"내 눈속에 너 니 눈속에 나 우린 야경처럼 반짝거리네 나는 널 채우는 샴페인 너는 날 깨우는 카페인 그대와 나의 밤이 아름다운 밤이 영원하도록 집에 가지 말아요\"\n",
    "\n",
    "masked_a, masked_b, recovered_a, recovered_b, is_next_prob = mask_segment_b_word_level_from_tokenized(\n",
    "    test_text, pre_train_model, vocab, mask_prob=0.1, mask_ab='all',\n",
    ")\n",
    "\n",
    "print(\"원본 A 문장:\\n\", string_a)\n",
    "print(\"원본 B 문장:\\n\", string_c)\n",
    "print(\"\\n 마스킹 A 문장:\\n\", masked_a)\n",
    "print(\" 마스킹 B 문장:\\n\", masked_b)\n",
    "print(\"\\n 복원된 A 문장:\\n\", recovered_a)\n",
    "print(\"복원된 B 문장:\\n\", recovered_b)\n",
    "\n",
    "print(f\"\\n NSP 확률 (두 문장이 이어질 확률): {is_next_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c27faa",
   "metadata": {},
   "source": [
    "다른 도메인의 문장을 nsp추론해본 경우, 예측의 방향성은 올바르나 역시 높은 편향을 가지고 예측하지는 못하는 모습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d3b9e",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "BERT를 포함하여 transformer 기반의 여러 모델들을 공부했는데,\n",
    "\n",
    "코드가 주어졌음에도 불구하고, 그 코드를 이해하는 부분에서 많은 어려움이 느껴졌습니다.\n",
    "\n",
    "이번 프로젝트의 경우 노드의 코드를 거의 그대로 사용해서 큰 어려움은 없었지만\n",
    "\n",
    "앞으로 만나볼 복잡한 모델을 잘 분석하는 요령을 터득하고싶다는 생각이 많이 들었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
